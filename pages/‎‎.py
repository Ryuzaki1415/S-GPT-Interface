import streamlit as st

st.title("Inference Hyperparameter documentation")
st.markdown('''

### top_k (Top K Sampling)
- **Description:** During sampling, only the top K most likely tokens are considered at each step, where K is the value of this hyperparameter. This technique is used to prevent the model from sampling from very unlikely tokens, which can improve the diversity of generated text.
- **Common Standard Value:** 50

### temperature (Temperature Scaling)
- **Description:** Temperature scaling is applied during sampling to control the randomness of the generated text. It adjusts the probability distribution of the model's output logits before sampling from it. Lower temperatures result in more conservative, high-confidence samples, while higher temperatures lead to more diverse, exploratory samples.
- **Common Standard Value:** 0.7

### max_new_tokens (Maximum Tokens Generated)
- **Description:** Specifies the maximum number of tokens (words or subwords) generated by the model during text generation. This helps control the length of the generated text.
- **Common Standard Value:** 100

### num_samples (Number of Samples)
- **Description:** Determines the number of independent samples to generate from the model during text generation. Generating multiple samples can help evaluate the diversity and quality of the model's output.
- **Common Standard Value:** 3

            ''')
st.divider()
if st.button("Home"):
    st.switch_page("HoMe.py")
if st.button("Generate!"):
    st.switch_page("pages/Inference.py")